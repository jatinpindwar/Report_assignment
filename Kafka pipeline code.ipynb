{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer,Consumer, KafkaError\n",
    "import happybase\n",
    "from pyspark.sql import SparkSession\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a715b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka broker(s) configuration\n",
    "kafka_brokers = 'localhost:9092'  # Replace with your Kafka broker(s) info\n",
    "\n",
    "# Kafka topic where clickstream data will be sent\n",
    "topic = 'stream_topic'  # Replace with your desired topic name\n",
    "\n",
    "def report_status(error, msg):\n",
    "    if error is not None:\n",
    "        print(f\"Message delivery failed: {error}\")\n",
    "    else:\n",
    "        print(f\"Message delivered to topic: {msg.topic()} - Partition: {msg.partition()} - Offset: {msg.offset()}\")\n",
    "\n",
    "# Create Kafka producer configuration\n",
    "producer_config = {\n",
    "    'bootstrap.servers': kafka_brokers,\n",
    "    'client.id': 'clickstream_id',  # Replace with a unique client ID\n",
    "}\n",
    "\n",
    "# Create Kafka producer instance\n",
    "producer = Producer(producer_config)\n",
    "\n",
    "# Sample clickstream data \n",
    "random_data = [\n",
    "    \"click1,user1,timestamp1,url1,country1,city1,browser1,os1,device1\",\n",
    "    \"click2,user2,timestamp2,url2,country2,city2,browser2,os2,device2\",\n",
    "    \"click3,user3,timestamp3,url1,country1,city3,browser1,os3,device1\",]\n",
    "\n",
    "# Send clickstream data to Kafka topic\n",
    "for data in random_data:\n",
    "    producer.produce(topic, value=data, callback=report_status)\n",
    "\n",
    "# Wait for any outstanding messages to be delivered\n",
    "producer.flush()\n",
    "\n",
    "# Close the Kafka producer\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7053ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Ingest Clickstream Data from Kafka\n",
    "kafka_config = {\n",
    "    'bootstrap.servers': kafka_brokers,  \n",
    "    'group.id': 'clickstream_group_id',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "consumer = Consumer(kafka_config)\n",
    "consumer.subscribe([topic])\n",
    "\n",
    "# Step 2: Store the ingested data in Apache HBase\n",
    "hbase_host = 'localhost'  \n",
    "hbase_table = 'geo_table' \n",
    "\n",
    "connection = happybase.Connection(hbase_host)\n",
    "table = connection.table(hbase_table)\n",
    "\n",
    "for msg in consumer:\n",
    "    if not msg.error():\n",
    "        click_data = msg.value().split(',')  # Assuming the clickstream data is comma-separated\n",
    "        row_key = click_data[0]  # Assuming the unique identifier is the first value\n",
    "\n",
    "        # Assuming click_data contains user ID, timestamp, and URL in order\n",
    "        click_info = {\n",
    "            'click_data:user_id': click_data[1],\n",
    "            'click_data:timestamp': click_data[2],\n",
    "            'click_data:url': click_data[3],\n",
    "            'geo_data:country': click_data[4],  # Assuming country is the 5th value\n",
    "            'geo_data:city': click_data[5],  # Assuming city is the 6th value\n",
    "            'user_agent_data:browser': click_data[6],  # Assuming browser is the 7th value\n",
    "            'user_agent_data:os': click_data[7],  # Assuming OS is the 8th value\n",
    "            'user_agent_data:device': click_data[8]  # Assuming device is the 9th value\n",
    "        }\n",
    "\n",
    "        table.put(row_key, click_info)\n",
    "\n",
    "# Step 3: Periodically process the stored clickstream data with Apache Spark\n",
    "spark = SparkSession.builder.appName(\"ClickstreamProcessing\").getOrCreate()\n",
    "\n",
    "# HBase data is preprocessed into a DataFrame with columns 'url', 'country', 'time_spent', and 'user_id'\n",
    "# Use Spark SQL to aggregate data by 'url' and 'country' and calculate number of clicks, unique users, and average time spent\n",
    "\n",
    "processed_data = spark.sql(\"\"\"\n",
    "    SELECT url, country,\n",
    "           COUNT(*) as num_clicks,\n",
    "           COUNT(DISTINCT user_id) as num_unique_users,\n",
    "           AVG(time_spent) as avg_time_spent\n",
    "    FROM clickstream_data\n",
    "    GROUP BY url, country\n",
    "\"\"\")\n",
    "\n",
    "# Step 4: Index the processed data in Elasticsearch\n",
    "es_host = 'localhost'\n",
    "es_index = 'clickstream_data_index' \n",
    "\n",
    "es = Elasticsearch([{'host': es_host, 'port': 9200}])\n",
    "processed_data.write.format(\"org.elasticsearch.spark.sql\").option(\"es.resource\", es_index).save()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "# Close the Kafka consumer\n",
    "consumer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
